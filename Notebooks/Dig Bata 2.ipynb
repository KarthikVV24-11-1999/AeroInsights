{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fb8049a-e57d-4dae-ad2b-3c7f3c722489",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd65960d-b29d-4c48-9bd1-ca493249715d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/09 19:07:53 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read CSV\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read CSV into DataFrame\n",
    "# prices = spark.read.csv(\"itineraries.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50377d3-fc9f-4af2-8be4-0bdf3cd2c9b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:=================================>                     (71 + 8) / 116]\r"
     ]
    }
   ],
   "source": [
    "delays_all = spark.read.csv(\"delays/raw/*.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dad1d7-4113-4e47-8698-391e55bd9266",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8611a7-6a27-4f8d-9e9a-dd1cb7237dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "airline_all = spark.read.csv(\"delays/delays_again/*.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d442eb9f-02e6-4867-af9f-e12a1f64d958",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = airline_all.rdd.map(lambda row: (row.Operating_Airline, row.Airline)).collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b0982b-47f2-4b3c-97f8-629dc1e3a299",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e21733-86b7-4223-985d-f965ef56da5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6771f88e-6d3c-4a99-be88-68ed3bcf209f",
   "metadata": {},
   "outputs": [],
   "source": [
    "delays_all.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e12e609-358c-4f14-ae28-88bcf46d3921",
   "metadata": {},
   "outputs": [],
   "source": [
    "delays_all.select(\"Operating_Airline \").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aca3707-f3f8-457a-8673-846c1c372b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "cancelled_percentage_df = delays_all.groupBy('Operating_Airline ').agg(\n",
    "    (avg('Cancelled') * 100).alias('Cancelled_Percentage')\n",
    ")\n",
    "\n",
    "# Show the result\n",
    "cancelled_percentage_df.limit(100).toPandas().head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f680f6-23cf-427c-9a12-244b84dc16e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect the DataFrame as a list of rows\n",
    "cancelled_percentage_list = cancelled_percentage_df.collect()\n",
    "\n",
    "# Convert the list of rows to a dictionary with the airline as the key\n",
    "cancelled_percentage_dict = {row['Operating_Airline ']: row['Cancelled_Percentage'] for row in cancelled_percentage_list}\n",
    "\n",
    "# Print or use the dictionary as needed\n",
    "print(cancelled_percentage_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53739bfe-a8d1-4c0d-97be-5fb2bf6be707",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "diverted_percentage_df = delays_all.groupBy('Operating_Airline ').agg(\n",
    "    (avg('Diverted') * 100).alias('Diverted_Percentage')\n",
    ")\n",
    "\n",
    "# Show the result\n",
    "diverted_percentage_df.limit(100).toPandas().head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2a88bf-ce0c-41a7-a566-220c43bc10ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect the DataFrame as a list of rows\n",
    "diverted_percentage_list = diverted_percentage_df.collect()\n",
    "\n",
    "# Convert the list of rows to a dictionary with the airline as the key\n",
    "diverted_percentage_dict = {row['Operating_Airline ']: row['Diverted_Percentage'] for row in diverted_percentage_list}\n",
    "\n",
    "# Print or use the dictionary as needed\n",
    "print(diverted_percentage_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cac133d-f839-46bc-b907-0f8ca8a09545",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "delays_all.select(col('Operating_Airline '), col('Cancelled')).limit(10).toPandas().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7837468d-cef1-4e55-8ca6-af67f32cf51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |                    Delta| -> Delta Air Lines Inc.\n",
    "# |             Boutique Air| -> Compass Airlines\n",
    "# |             Key Lime Air| -> Peninsula Airways Inc\n",
    "# |     Sun Country Airlines| -> SkyWest Airlines Inc\n",
    "# |         Contour Airlines| -> Mesa Airlines Inc\n",
    "# |                   United| -> United Air Lines Inc.\n",
    "# |                 Cape Air|\n",
    "# |        Frontier Airlines| -> Frontier Airlines Inc. \n",
    "# |        American Airlines| -> American Airlines Inc.\n",
    "# |          JetBlue Airways|\n",
    "# |          Alaska Airlines| -> Alaska Airlines Inc.\n",
    "# |          Spirit Airlines| -> Spirit Airlines Inc.\n",
    "# |     Southern Airways ...| -> Southwest Airlines Co\n",
    "# |        Hawaiian Airlines| -> Hawaiian Airlines Inc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b90c50f-1a6e-4e6f-ab58-8449a3134354",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(set(list(dictionary.values()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb10004-1291-4fa0-bcbe-0239fe76f554",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import weekofyear, to_date\n",
    "\n",
    "delays_all_week = delays_all.withColumn(\"FlightWeek\", weekofyear(\"FlightDate\"))\n",
    "delays_all_week.limit(100).toPandas().head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85e310f-e710-44d1-aa49-237a81b9e957",
   "metadata": {},
   "outputs": [],
   "source": [
    "delays_all_week = delays_all_week.filter(delays_all_week.Cancelled != 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc01ec5-ca56-470f-ad04-7372aec0e445",
   "metadata": {},
   "outputs": [],
   "source": [
    "delays_all_week = delays_all_week.withColumnRenamed(\"Operating_Airline \", \"Operating_Airline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5585a7a5-48ba-4ba5-b0d3-e1ee376e875a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\n",
    "    'Origin',\n",
    "    \"Operating_Airline\",\n",
    "    'OriginCityName',\n",
    "    'Dest',\n",
    "    'DestCityName',\n",
    "    'CRSDepTime',\n",
    "    'DepDelay',\n",
    "    'DepDelayMinutes',\n",
    "    'TaxiOut',\n",
    "    'TaxiIn',\n",
    "    'CRSArrTime',\n",
    "    'ArrDelay',\n",
    "    'Distance',\n",
    "    'CarrierDelay',\n",
    "    'WeatherDelay',\n",
    "    'NASDelay',\n",
    "    'SecurityDelay',\n",
    "    'LateAircraftDelay',\n",
    "    'DivArrDelay',\n",
    "    'FlightWeek',\n",
    "    \"Year\"\n",
    "]\n",
    "\n",
    "delays_preprocessed = delays_all_week[cols]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dab670-b300-4325-93e2-cfdb94bb3b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "delays_preprocessed.limit(100).toPandas().head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2767f5-a446-45f7-9d2b-823590b84a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "delays_preprocessed = delays_preprocessed.fillna(0, subset=[\"CarrierDelay\", \"WeatherDelay\", \"NASDelay\", \"SecurityDelay\", \"LateAircraftDelay\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78d132b-b47d-492e-a3f8-8da2ffb00467",
   "metadata": {},
   "outputs": [],
   "source": [
    "delays_preprocessed.limit(100).toPandas().head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6db97f-1612-400f-99ef-58ae7b20d2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# Import val from col2 to col1 where col1's val is null and col2 is NOT NULL\n",
    "delays_preprocessed = delays_preprocessed.withColumn(\"ArrDelay\", when(col(\"ArrDelay\").isNull() & col(\"DivArrDelay\").isNotNull(), col(\"DivArrDelay\")).otherwise(col(\"ArrDelay\")))\n",
    "# Import val from col1 to col2 where col2's val is null and col1 is NOT NULL\n",
    "delays_preprocessed = delays_preprocessed.withColumn(\"DivArrDelay\", when(col(\"DivArrDelay\").isNull() & col(\"ArrDelay\").isNotNull(), col(\"ArrDelay\")).otherwise(col(\"DivArrDelay\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35c079a-9f01-43d5-a595-817d00f1e301",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a7152ccf-de4b-4d40-8e2b-07c8b2c94443",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Origin</th>\n",
       "      <th>Operating_Airline</th>\n",
       "      <th>OriginCityName</th>\n",
       "      <th>Dest</th>\n",
       "      <th>DestCityName</th>\n",
       "      <th>CRSDepTime</th>\n",
       "      <th>DepDelay</th>\n",
       "      <th>DepDelayMinutes</th>\n",
       "      <th>TaxiOut</th>\n",
       "      <th>TaxiIn</th>\n",
       "      <th>...</th>\n",
       "      <th>ArrDelay</th>\n",
       "      <th>Distance</th>\n",
       "      <th>CarrierDelay</th>\n",
       "      <th>WeatherDelay</th>\n",
       "      <th>NASDelay</th>\n",
       "      <th>SecurityDelay</th>\n",
       "      <th>LateAircraftDelay</th>\n",
       "      <th>DivArrDelay</th>\n",
       "      <th>FlightWeek</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABY</td>\n",
       "      <td>9E</td>\n",
       "      <td>Albany, GA</td>\n",
       "      <td>ATL</td>\n",
       "      <td>Atlanta, GA</td>\n",
       "      <td>1202</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABY</td>\n",
       "      <td>9E</td>\n",
       "      <td>Albany, GA</td>\n",
       "      <td>ATL</td>\n",
       "      <td>Atlanta, GA</td>\n",
       "      <td>1202</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABY</td>\n",
       "      <td>9E</td>\n",
       "      <td>Albany, GA</td>\n",
       "      <td>ATL</td>\n",
       "      <td>Atlanta, GA</td>\n",
       "      <td>1202</td>\n",
       "      <td>-9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABY</td>\n",
       "      <td>9E</td>\n",
       "      <td>Albany, GA</td>\n",
       "      <td>ATL</td>\n",
       "      <td>Atlanta, GA</td>\n",
       "      <td>1202</td>\n",
       "      <td>-12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABY</td>\n",
       "      <td>9E</td>\n",
       "      <td>Albany, GA</td>\n",
       "      <td>ATL</td>\n",
       "      <td>Atlanta, GA</td>\n",
       "      <td>1400</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>LGA</td>\n",
       "      <td>9E</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>SYR</td>\n",
       "      <td>Syracuse, NY</td>\n",
       "      <td>1810</td>\n",
       "      <td>-12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-25.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-25.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>LGA</td>\n",
       "      <td>9E</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>SYR</td>\n",
       "      <td>Syracuse, NY</td>\n",
       "      <td>1810</td>\n",
       "      <td>-12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-26.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-26.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>ATL</td>\n",
       "      <td>9E</td>\n",
       "      <td>Atlanta, GA</td>\n",
       "      <td>EWN</td>\n",
       "      <td>New Bern/Morehead/Beaufort, NC</td>\n",
       "      <td>1225</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>433.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>ATL</td>\n",
       "      <td>9E</td>\n",
       "      <td>Atlanta, GA</td>\n",
       "      <td>EWN</td>\n",
       "      <td>New Bern/Morehead/Beaufort, NC</td>\n",
       "      <td>1225</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>433.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>ATL</td>\n",
       "      <td>9E</td>\n",
       "      <td>Atlanta, GA</td>\n",
       "      <td>EWN</td>\n",
       "      <td>New Bern/Morehead/Beaufort, NC</td>\n",
       "      <td>1225</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>433.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Origin Operating_Airline OriginCityName Dest  \\\n",
       "0     ABY                9E     Albany, GA  ATL   \n",
       "1     ABY                9E     Albany, GA  ATL   \n",
       "2     ABY                9E     Albany, GA  ATL   \n",
       "3     ABY                9E     Albany, GA  ATL   \n",
       "4     ABY                9E     Albany, GA  ATL   \n",
       "..    ...               ...            ...  ...   \n",
       "95    LGA                9E   New York, NY  SYR   \n",
       "96    LGA                9E   New York, NY  SYR   \n",
       "97    ATL                9E    Atlanta, GA  EWN   \n",
       "98    ATL                9E    Atlanta, GA  EWN   \n",
       "99    ATL                9E    Atlanta, GA  EWN   \n",
       "\n",
       "                      DestCityName  CRSDepTime  DepDelay  DepDelayMinutes  \\\n",
       "0                      Atlanta, GA        1202      -5.0              0.0   \n",
       "1                      Atlanta, GA        1202      -5.0              0.0   \n",
       "2                      Atlanta, GA        1202      -9.0              0.0   \n",
       "3                      Atlanta, GA        1202     -12.0              0.0   \n",
       "4                      Atlanta, GA        1400      -5.0              0.0   \n",
       "..                             ...         ...       ...              ...   \n",
       "95                    Syracuse, NY        1810     -12.0              0.0   \n",
       "96                    Syracuse, NY        1810     -12.0              0.0   \n",
       "97  New Bern/Morehead/Beaufort, NC        1225      -3.0              0.0   \n",
       "98  New Bern/Morehead/Beaufort, NC        1225       6.0              6.0   \n",
       "99  New Bern/Morehead/Beaufort, NC        1225      -3.0              0.0   \n",
       "\n",
       "    TaxiOut  TaxiIn  ...  ArrDelay  Distance  CarrierDelay  WeatherDelay  \\\n",
       "0      14.0     7.0  ...      -8.0     145.0           0.0           0.0   \n",
       "1      13.0    12.0  ...      -6.0     145.0           0.0           0.0   \n",
       "2      18.0    11.0  ...      -2.0     145.0           0.0           0.0   \n",
       "3      17.0    11.0  ...     -11.0     145.0           0.0           0.0   \n",
       "4      17.0    11.0  ...      -1.0     145.0           0.0           0.0   \n",
       "..      ...     ...  ...       ...       ...           ...           ...   \n",
       "95     24.0     6.0  ...     -25.0     198.0           0.0           0.0   \n",
       "96     24.0     5.0  ...     -26.0     198.0           0.0           0.0   \n",
       "97     17.0     3.0  ...      -4.0     433.0           0.0           0.0   \n",
       "98     26.0     6.0  ...      15.0     433.0           0.0           0.0   \n",
       "99     18.0     4.0  ...      -6.0     433.0           0.0           0.0   \n",
       "\n",
       "    NASDelay  SecurityDelay  LateAircraftDelay  DivArrDelay  FlightWeek  Year  \n",
       "0        0.0            0.0                0.0         -8.0           4  2018  \n",
       "1        0.0            0.0                0.0         -6.0           4  2018  \n",
       "2        0.0            0.0                0.0         -2.0           4  2018  \n",
       "3        0.0            0.0                0.0        -11.0           4  2018  \n",
       "4        0.0            0.0                0.0         -1.0           4  2018  \n",
       "..       ...            ...                ...          ...         ...   ...  \n",
       "95       0.0            0.0                0.0        -25.0           3  2018  \n",
       "96       0.0            0.0                0.0        -26.0           4  2018  \n",
       "97       0.0            0.0                0.0         -4.0           1  2018  \n",
       "98       9.0            0.0                6.0         15.0           1  2018  \n",
       "99       0.0            0.0                0.0         -6.0           1  2018  \n",
       "\n",
       "[100 rows x 21 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delays_preprocessed.limit(100).toPandas().head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fcbc91d4-239c-4eee-a7c8-28a94b29f40c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9465"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delays_preprocessed.filter(delays_preprocessed['ArrDelay'].isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d3a23977-5909-403c-902e-0030606f7246",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import mean\n",
    "\n",
    "mean_value = delays_preprocessed.agg(mean('ArrDelay')).collect()[0][0]\n",
    "delays_preprocessed = delays_preprocessed.fillna(mean_value, subset=['ArrDelay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cf1bc966-45f7-4056-bbbd-dd9134c661d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delays_preprocessed.filter(delays_preprocessed['ArrDelay'].isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8b9a9d77-1044-46f6-9ac3-c8df5a0df6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "delays_preprocessed = delays_preprocessed.drop(\"DivArrDelay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "689260b7-195c-4d7a-ae75-1a6e8fd1846d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Origin',\n",
       " 'Operating_Airline',\n",
       " 'OriginCityName',\n",
       " 'Dest',\n",
       " 'DestCityName',\n",
       " 'CRSDepTime',\n",
       " 'DepDelay',\n",
       " 'DepDelayMinutes',\n",
       " 'TaxiOut',\n",
       " 'TaxiIn',\n",
       " 'CRSArrTime',\n",
       " 'ArrDelay',\n",
       " 'Distance',\n",
       " 'CarrierDelay',\n",
       " 'WeatherDelay',\n",
       " 'NASDelay',\n",
       " 'SecurityDelay',\n",
       " 'LateAircraftDelay',\n",
       " 'FlightWeek',\n",
       " 'Year']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delays_preprocessed.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "79989837-55d1-437f-bcbf-8d640645a233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- Operating_Airline: string (nullable = true)\n",
      " |-- OriginCityName: string (nullable = true)\n",
      " |-- Dest: string (nullable = true)\n",
      " |-- DestCityName: string (nullable = true)\n",
      " |-- CRSDepTime: integer (nullable = true)\n",
      " |-- DepDelay: double (nullable = true)\n",
      " |-- DepDelayMinutes: double (nullable = true)\n",
      " |-- TaxiOut: double (nullable = true)\n",
      " |-- TaxiIn: double (nullable = true)\n",
      " |-- CRSArrTime: integer (nullable = true)\n",
      " |-- ArrDelay: double (nullable = false)\n",
      " |-- Distance: double (nullable = true)\n",
      " |-- CarrierDelay: double (nullable = false)\n",
      " |-- WeatherDelay: double (nullable = false)\n",
      " |-- NASDelay: double (nullable = false)\n",
      " |-- SecurityDelay: double (nullable = false)\n",
      " |-- LateAircraftDelay: double (nullable = false)\n",
      " |-- FlightWeek: integer (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "delays_preprocessed.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fad2cb3a-4c5d-4683-9d18-d19c28ea9d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "delays_preprocessed = delays_preprocessed.withColumn(\n",
    "    \"depDaypart\",\n",
    "    when((delays_preprocessed[\"CRSDepTime\"] >= 600) & (delays_preprocessed[\"CRSDepTime\"] < 1200), \"morning\")\n",
    "    .when((delays_preprocessed[\"CRSDepTime\"] >= 1200) & (delays_preprocessed[\"CRSDepTime\"] < 1600), \"afternoon\")\n",
    "    .when((delays_preprocessed[\"CRSDepTime\"] >= 1600) & (delays_preprocessed[\"CRSDepTime\"] < 1900), \"evening\")\n",
    "    .otherwise(\"night\")\n",
    ")\n",
    "\n",
    "delays_preprocessed = delays_preprocessed.withColumn(\n",
    "    \"arrDaypart\",\n",
    "    when((delays_preprocessed[\"CRSArrTime\"] >= 600) & (delays_preprocessed[\"CRSArrTime\"] < 1200), \"morning\")\n",
    "    .when((delays_preprocessed[\"CRSArrTime\"] >= 1200) & (delays_preprocessed[\"CRSArrTime\"] < 1600), \"afternoon\")\n",
    "    .when((delays_preprocessed[\"CRSArrTime\"] >= 1600) & (delays_preprocessed[\"CRSArrTime\"] < 1900), \"evening\")\n",
    "    .otherwise(\"night\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4bb0e67c-e89a-4672-94d6-e18317a4afb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRSDepTime</th>\n",
       "      <th>depDaypart</th>\n",
       "      <th>CRSArrTime</th>\n",
       "      <th>arrDaypart</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1202</td>\n",
       "      <td>afternoon</td>\n",
       "      <td>1304</td>\n",
       "      <td>afternoon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1202</td>\n",
       "      <td>afternoon</td>\n",
       "      <td>1304</td>\n",
       "      <td>afternoon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1202</td>\n",
       "      <td>afternoon</td>\n",
       "      <td>1304</td>\n",
       "      <td>afternoon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1202</td>\n",
       "      <td>afternoon</td>\n",
       "      <td>1304</td>\n",
       "      <td>afternoon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1400</td>\n",
       "      <td>afternoon</td>\n",
       "      <td>1500</td>\n",
       "      <td>afternoon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1202</td>\n",
       "      <td>afternoon</td>\n",
       "      <td>1304</td>\n",
       "      <td>afternoon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1202</td>\n",
       "      <td>afternoon</td>\n",
       "      <td>1304</td>\n",
       "      <td>afternoon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1202</td>\n",
       "      <td>afternoon</td>\n",
       "      <td>1304</td>\n",
       "      <td>afternoon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1202</td>\n",
       "      <td>afternoon</td>\n",
       "      <td>1304</td>\n",
       "      <td>afternoon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1037</td>\n",
       "      <td>morning</td>\n",
       "      <td>1137</td>\n",
       "      <td>morning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1037</td>\n",
       "      <td>morning</td>\n",
       "      <td>1137</td>\n",
       "      <td>morning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1037</td>\n",
       "      <td>morning</td>\n",
       "      <td>1137</td>\n",
       "      <td>morning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1037</td>\n",
       "      <td>morning</td>\n",
       "      <td>1136</td>\n",
       "      <td>morning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1037</td>\n",
       "      <td>morning</td>\n",
       "      <td>1137</td>\n",
       "      <td>morning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1037</td>\n",
       "      <td>morning</td>\n",
       "      <td>1137</td>\n",
       "      <td>morning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1037</td>\n",
       "      <td>morning</td>\n",
       "      <td>1137</td>\n",
       "      <td>morning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1037</td>\n",
       "      <td>morning</td>\n",
       "      <td>1137</td>\n",
       "      <td>morning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1037</td>\n",
       "      <td>morning</td>\n",
       "      <td>1137</td>\n",
       "      <td>morning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1037</td>\n",
       "      <td>morning</td>\n",
       "      <td>1137</td>\n",
       "      <td>morning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1037</td>\n",
       "      <td>morning</td>\n",
       "      <td>1136</td>\n",
       "      <td>morning</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    CRSDepTime depDaypart  CRSArrTime arrDaypart\n",
       "0         1202  afternoon        1304  afternoon\n",
       "1         1202  afternoon        1304  afternoon\n",
       "2         1202  afternoon        1304  afternoon\n",
       "3         1202  afternoon        1304  afternoon\n",
       "4         1400  afternoon        1500  afternoon\n",
       "5         1202  afternoon        1304  afternoon\n",
       "6         1202  afternoon        1304  afternoon\n",
       "7         1202  afternoon        1304  afternoon\n",
       "8         1202  afternoon        1304  afternoon\n",
       "9         1037    morning        1137    morning\n",
       "10        1037    morning        1137    morning\n",
       "11        1037    morning        1137    morning\n",
       "12        1037    morning        1136    morning\n",
       "13        1037    morning        1137    morning\n",
       "14        1037    morning        1137    morning\n",
       "15        1037    morning        1137    morning\n",
       "16        1037    morning        1137    morning\n",
       "17        1037    morning        1137    morning\n",
       "18        1037    morning        1137    morning\n",
       "19        1037    morning        1136    morning"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delays_preprocessed.select(col(\"CRSDepTime\"), col(\"depDaypart\"), col(\"CRSArrTime\"), col(\"arrDaypart\")).limit(20).toPandas().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "58061ce2-d2ee-468d-af88-e423fe6e3b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "delays_preprocessed = delays_preprocessed.drop(\"CRSDepTime\").drop(\"CRSArrTime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ad176b14-1005-4618-b66b-94842296fd01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Origin',\n",
       " 'Operating_Airline',\n",
       " 'OriginCityName',\n",
       " 'Dest',\n",
       " 'DestCityName',\n",
       " 'DepDelay',\n",
       " 'DepDelayMinutes',\n",
       " 'TaxiOut',\n",
       " 'TaxiIn',\n",
       " 'ArrDelay',\n",
       " 'Distance',\n",
       " 'CarrierDelay',\n",
       " 'WeatherDelay',\n",
       " 'NASDelay',\n",
       " 'SecurityDelay',\n",
       " 'LateAircraftDelay',\n",
       " 'FlightWeek',\n",
       " 'Year',\n",
       " 'depDaypart',\n",
       " 'arrDaypart']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delays_preprocessed.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0f572af0-13de-4353-9c1d-7f92c9160945",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+----+------------+--------+---------------+-------+------+-----------------+--------+------------+------------+--------+-------------+-----------------+----------+----+----------+----------+------------+--------------------+----------+------------------+----------------+----------------+----------------+-----------------+---------------------+-----------------+-------------------+-----------------+-----------------+-----------------+\n",
      "|Origin|OriginCityName|Dest|DestCityName|DepDelay|DepDelayMinutes|TaxiOut|TaxiIn|         ArrDelay|Distance|CarrierDelay|WeatherDelay|NASDelay|SecurityDelay|LateAircraftDelay|FlightWeek|Year|depDaypart|arrDaypart|Origin_index|OriginCityName_index|Dest_index|DestCityName_index|FlightWeek_index|arrDaypart_index|depDaypart_index|    Origin_onehot|OriginCityName_onehot|      Dest_onehot|DestCityName_onehot|FlightWeek_onehot|arrDaypart_onehot|depDaypart_onehot|\n",
      "+------+--------------+----+------------+--------+---------------+-------+------+-----------------+--------+------------+------------+--------+-------------+-----------------+----------+----+----------+----------+------------+--------------------+----------+------------------+----------------+----------------+----------------+-----------------+---------------------+-----------------+-------------------+-----------------+-----------------+-----------------+\n",
      "|   ABY|    Albany, GA| ATL| Atlanta, GA|    -5.0|            0.0|   14.0|   7.0|             -8.0|   145.0|         0.0|         0.0|     0.0|          0.0|              0.0|         4|2018| afternoon| afternoon|       272.0|               266.0|       0.0|               1.0|             8.0|             2.0|             1.0|(388,[272],[1.0])|    (381,[266],[1.0])|  (388,[0],[1.0])|    (381,[1],[1.0])|   (53,[8],[1.0])|    (4,[2],[1.0])|    (4,[1],[1.0])|\n",
      "|   ABY|    Albany, GA| ATL| Atlanta, GA|    -5.0|            0.0|   13.0|  12.0|             -6.0|   145.0|         0.0|         0.0|     0.0|          0.0|              0.0|         4|2018| afternoon| afternoon|       272.0|               266.0|       0.0|               1.0|             8.0|             2.0|             1.0|(388,[272],[1.0])|    (381,[266],[1.0])|  (388,[0],[1.0])|    (381,[1],[1.0])|   (53,[8],[1.0])|    (4,[2],[1.0])|    (4,[1],[1.0])|\n",
      "|   ABY|    Albany, GA| ATL| Atlanta, GA|    -9.0|            0.0|   18.0|  11.0|             -2.0|   145.0|         0.0|         0.0|     0.0|          0.0|              0.0|         4|2018| afternoon| afternoon|       272.0|               266.0|       0.0|               1.0|             8.0|             2.0|             1.0|(388,[272],[1.0])|    (381,[266],[1.0])|  (388,[0],[1.0])|    (381,[1],[1.0])|   (53,[8],[1.0])|    (4,[2],[1.0])|    (4,[1],[1.0])|\n",
      "|   ABY|    Albany, GA| ATL| Atlanta, GA|   -12.0|            0.0|   17.0|  11.0|            -11.0|   145.0|         0.0|         0.0|     0.0|          0.0|              0.0|         4|2018| afternoon| afternoon|       272.0|               266.0|       0.0|               1.0|             8.0|             2.0|             1.0|(388,[272],[1.0])|    (381,[266],[1.0])|  (388,[0],[1.0])|    (381,[1],[1.0])|   (53,[8],[1.0])|    (4,[2],[1.0])|    (4,[1],[1.0])|\n",
      "|   ABY|    Albany, GA| ATL| Atlanta, GA|    -5.0|            0.0|   17.0|  11.0|             -1.0|   145.0|         0.0|         0.0|     0.0|          0.0|              0.0|         4|2018| afternoon| afternoon|       272.0|               266.0|       0.0|               1.0|             8.0|             2.0|             1.0|(388,[272],[1.0])|    (381,[266],[1.0])|  (388,[0],[1.0])|    (381,[1],[1.0])|   (53,[8],[1.0])|    (4,[2],[1.0])|    (4,[1],[1.0])|\n",
      "|   ABY|    Albany, GA| ATL| Atlanta, GA|    NULL|           NULL|   34.0|  13.0|             22.0|   145.0|         0.0|         0.0|    22.0|          0.0|              0.0|         4|2018| afternoon| afternoon|       272.0|               266.0|       0.0|               1.0|             8.0|             2.0|             1.0|(388,[272],[1.0])|    (381,[266],[1.0])|  (388,[0],[1.0])|    (381,[1],[1.0])|   (53,[8],[1.0])|    (4,[2],[1.0])|    (4,[1],[1.0])|\n",
      "|   ABY|    Albany, GA| ATL| Atlanta, GA|     2.0|            2.0|   15.0|  10.0|             -1.0|   145.0|         0.0|         0.0|     0.0|          0.0|              0.0|         5|2018| afternoon| afternoon|       272.0|               266.0|       0.0|               1.0|            20.0|             2.0|             1.0|(388,[272],[1.0])|    (381,[266],[1.0])|  (388,[0],[1.0])|    (381,[1],[1.0])|  (53,[20],[1.0])|    (4,[2],[1.0])|    (4,[1],[1.0])|\n",
      "|   ABY|    Albany, GA| ATL| Atlanta, GA|    -9.0|            0.0|    7.0|  11.0|             -9.0|   145.0|         0.0|         0.0|     0.0|          0.0|              0.0|         5|2018| afternoon| afternoon|       272.0|               266.0|       0.0|               1.0|            20.0|             2.0|             1.0|(388,[272],[1.0])|    (381,[266],[1.0])|  (388,[0],[1.0])|    (381,[1],[1.0])|  (53,[20],[1.0])|    (4,[2],[1.0])|    (4,[1],[1.0])|\n",
      "|   ABY|    Albany, GA| ATL| Atlanta, GA|    -9.0|            0.0|   26.0|   8.0|4.131181203257642|   145.0|         0.0|         0.0|     0.0|          0.0|              0.0|         5|2018| afternoon| afternoon|       272.0|               266.0|       0.0|               1.0|            20.0|             2.0|             1.0|(388,[272],[1.0])|    (381,[266],[1.0])|  (388,[0],[1.0])|    (381,[1],[1.0])|  (53,[20],[1.0])|    (4,[2],[1.0])|    (4,[1],[1.0])|\n",
      "|   ATL|   Atlanta, GA| ABY|  Albany, GA|    24.0|           24.0|   23.0|   3.0|             22.0|   145.0|        22.0|         0.0|     0.0|          0.0|              0.0|         1|2018|   morning|   morning|         0.0|                 1.0|     271.0|             265.0|             7.0|             1.0|             0.0|  (388,[0],[1.0])|      (381,[1],[1.0])|(388,[271],[1.0])|  (381,[265],[1.0])|   (53,[7],[1.0])|    (4,[1],[1.0])|    (4,[0],[1.0])|\n",
      "|   ATL|   Atlanta, GA| ABY|  Albany, GA|    -5.0|            0.0|   23.0|   3.0|            -12.0|   145.0|         0.0|         0.0|     0.0|          0.0|              0.0|         1|2018|   morning|   morning|         0.0|                 1.0|     271.0|             265.0|             7.0|             1.0|             0.0|  (388,[0],[1.0])|      (381,[1],[1.0])|(388,[271],[1.0])|  (381,[265],[1.0])|   (53,[7],[1.0])|    (4,[1],[1.0])|    (4,[0],[1.0])|\n",
      "|   ATL|   Atlanta, GA| ABY|  Albany, GA|    -5.0|            0.0|   20.0|   3.0|            -13.0|   145.0|         0.0|         0.0|     0.0|          0.0|              0.0|         1|2018|   morning|   morning|         0.0|                 1.0|     271.0|             265.0|             7.0|             1.0|             0.0|  (388,[0],[1.0])|      (381,[1],[1.0])|(388,[271],[1.0])|  (381,[265],[1.0])|   (53,[7],[1.0])|    (4,[1],[1.0])|    (4,[0],[1.0])|\n",
      "|   ATL|   Atlanta, GA| ABY|  Albany, GA|    -5.0|            0.0|   18.0|   2.0|            -10.0|   145.0|         0.0|         0.0|     0.0|          0.0|              0.0|         1|2018|   morning|   morning|         0.0|                 1.0|     271.0|             265.0|             7.0|             1.0|             0.0|  (388,[0],[1.0])|      (381,[1],[1.0])|(388,[271],[1.0])|  (381,[265],[1.0])|   (53,[7],[1.0])|    (4,[1],[1.0])|    (4,[0],[1.0])|\n",
      "|   ATL|   Atlanta, GA| ABY|  Albany, GA|    -3.0|            0.0|   18.0|   3.0|            -13.0|   145.0|         0.0|         0.0|     0.0|          0.0|              0.0|         1|2018|   morning|   morning|         0.0|                 1.0|     271.0|             265.0|             7.0|             1.0|             0.0|  (388,[0],[1.0])|      (381,[1],[1.0])|(388,[271],[1.0])|  (381,[265],[1.0])|   (53,[7],[1.0])|    (4,[1],[1.0])|    (4,[0],[1.0])|\n",
      "|   ATL|   Atlanta, GA| ABY|  Albany, GA|    22.0|           22.0|   18.0|   4.0|             16.0|   145.0|         0.0|         0.0|     0.0|          0.0|             16.0|         2|2018|   morning|   morning|         0.0|                 1.0|     271.0|             265.0|             9.0|             1.0|             0.0|  (388,[0],[1.0])|      (381,[1],[1.0])|(388,[271],[1.0])|  (381,[265],[1.0])|   (53,[9],[1.0])|    (4,[1],[1.0])|    (4,[0],[1.0])|\n",
      "|   ATL|   Atlanta, GA| ABY|  Albany, GA|   105.0|          105.0|   20.0|   3.0|            105.0|   145.0|         0.0|         0.0|     0.0|          0.0|            105.0|         2|2018|   morning|   morning|         0.0|                 1.0|     271.0|             265.0|             9.0|             1.0|             0.0|  (388,[0],[1.0])|      (381,[1],[1.0])|(388,[271],[1.0])|  (381,[265],[1.0])|   (53,[9],[1.0])|    (4,[1],[1.0])|    (4,[0],[1.0])|\n",
      "|   ATL|   Atlanta, GA| ABY|  Albany, GA|   -10.0|            0.0|   19.0|   3.0|            -11.0|   145.0|         0.0|         0.0|     0.0|          0.0|              0.0|         2|2018|   morning|   morning|         0.0|                 1.0|     271.0|             265.0|             9.0|             1.0|             0.0|  (388,[0],[1.0])|      (381,[1],[1.0])|(388,[271],[1.0])|  (381,[265],[1.0])|   (53,[9],[1.0])|    (4,[1],[1.0])|    (4,[0],[1.0])|\n",
      "|   ATL|   Atlanta, GA| ABY|  Albany, GA|    -4.0|            0.0|   31.0|   3.0|              8.0|   145.0|         0.0|         0.0|     0.0|          0.0|              0.0|         2|2018|   morning|   morning|         0.0|                 1.0|     271.0|             265.0|             9.0|             1.0|             0.0|  (388,[0],[1.0])|      (381,[1],[1.0])|(388,[271],[1.0])|  (381,[265],[1.0])|   (53,[9],[1.0])|    (4,[1],[1.0])|    (4,[0],[1.0])|\n",
      "|   ATL|   Atlanta, GA| ABY|  Albany, GA|     7.0|            7.0|   16.0|   3.0|              1.0|   145.0|         0.0|         0.0|     0.0|          0.0|              0.0|         2|2018|   morning|   morning|         0.0|                 1.0|     271.0|             265.0|             9.0|             1.0|             0.0|  (388,[0],[1.0])|      (381,[1],[1.0])|(388,[271],[1.0])|  (381,[265],[1.0])|   (53,[9],[1.0])|    (4,[1],[1.0])|    (4,[0],[1.0])|\n",
      "|   ATL|   Atlanta, GA| ABY|  Albany, GA|    -3.0|            0.0|   16.0|   2.0|            -10.0|   145.0|         0.0|         0.0|     0.0|          0.0|              0.0|         2|2018|   morning|   morning|         0.0|                 1.0|     271.0|             265.0|             9.0|             1.0|             0.0|  (388,[0],[1.0])|      (381,[1],[1.0])|(388,[271],[1.0])|  (381,[265],[1.0])|   (53,[9],[1.0])|    (4,[1],[1.0])|    (4,[0],[1.0])|\n",
      "+------+--------------+----+------------+--------+---------------+-------+------+-----------------+--------+------------+------------+--------+-------------+-----------------+----------+----+----------+----------+------------+--------------------+----------+------------------+----------------+----------------+----------------+-----------------+---------------------+-----------------+-------------------+-----------------+-----------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\", handleInvalid=\"keep\") for column in [\"Origin\", \"OriginCityName\", \"Dest\", \"DestCityName\", \"FlightWeek\", \"arrDaypart\", \"depDaypart\"]]\n",
    "\n",
    "# Perform OneHotEncoding on the indexed columns\n",
    "encoders = [OneHotEncoder(inputCol=column+\"_index\", outputCol=column+\"_onehot\") for column in [\"Origin\", \"OriginCityName\", \"Dest\", \"DestCityName\", \"FlightWeek\", \"arrDaypart\", \"depDaypart\"]]\n",
    "\n",
    "# Create a pipeline to execute the indexers and encoders sequentially\n",
    "pipeline = Pipeline(stages=indexers + encoders)\n",
    "\n",
    "# Fit the pipeline to the data and transform the DataFrame\n",
    "model = pipeline.fit(delays_preprocessed)\n",
    "encoded_df = model.transform(delays_preprocessed)\n",
    "\n",
    "# Show the encoded DataFrame\n",
    "encoded_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "28b32434-5e24-4773-899d-bedc30ffb134",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Origin</th>\n",
       "      <th>OriginCityName</th>\n",
       "      <th>Dest</th>\n",
       "      <th>DestCityName</th>\n",
       "      <th>DepDelay</th>\n",
       "      <th>DepDelayMinutes</th>\n",
       "      <th>TaxiOut</th>\n",
       "      <th>TaxiIn</th>\n",
       "      <th>ArrDelay</th>\n",
       "      <th>Distance</th>\n",
       "      <th>...</th>\n",
       "      <th>FlightWeek_index</th>\n",
       "      <th>arrDaypart_index</th>\n",
       "      <th>depDaypart_index</th>\n",
       "      <th>Origin_onehot</th>\n",
       "      <th>OriginCityName_onehot</th>\n",
       "      <th>Dest_onehot</th>\n",
       "      <th>DestCityName_onehot</th>\n",
       "      <th>FlightWeek_onehot</th>\n",
       "      <th>arrDaypart_onehot</th>\n",
       "      <th>depDaypart_onehot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABY</td>\n",
       "      <td>Albany, GA</td>\n",
       "      <td>ATL</td>\n",
       "      <td>Atlanta, GA</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-8.000000</td>\n",
       "      <td>145.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...</td>\n",
       "      <td>(0.0, 0.0, 1.0, 0.0)</td>\n",
       "      <td>(0.0, 1.0, 0.0, 0.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABY</td>\n",
       "      <td>Albany, GA</td>\n",
       "      <td>ATL</td>\n",
       "      <td>Atlanta, GA</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-6.000000</td>\n",
       "      <td>145.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...</td>\n",
       "      <td>(0.0, 0.0, 1.0, 0.0)</td>\n",
       "      <td>(0.0, 1.0, 0.0, 0.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABY</td>\n",
       "      <td>Albany, GA</td>\n",
       "      <td>ATL</td>\n",
       "      <td>Atlanta, GA</td>\n",
       "      <td>-9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>145.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...</td>\n",
       "      <td>(0.0, 0.0, 1.0, 0.0)</td>\n",
       "      <td>(0.0, 1.0, 0.0, 0.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABY</td>\n",
       "      <td>Albany, GA</td>\n",
       "      <td>ATL</td>\n",
       "      <td>Atlanta, GA</td>\n",
       "      <td>-12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>-11.000000</td>\n",
       "      <td>145.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...</td>\n",
       "      <td>(0.0, 0.0, 1.0, 0.0)</td>\n",
       "      <td>(0.0, 1.0, 0.0, 0.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABY</td>\n",
       "      <td>Albany, GA</td>\n",
       "      <td>ATL</td>\n",
       "      <td>Atlanta, GA</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>145.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...</td>\n",
       "      <td>(0.0, 0.0, 1.0, 0.0)</td>\n",
       "      <td>(0.0, 1.0, 0.0, 0.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ABY</td>\n",
       "      <td>Albany, GA</td>\n",
       "      <td>ATL</td>\n",
       "      <td>Atlanta, GA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>145.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...</td>\n",
       "      <td>(0.0, 0.0, 1.0, 0.0)</td>\n",
       "      <td>(0.0, 1.0, 0.0, 0.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ABY</td>\n",
       "      <td>Albany, GA</td>\n",
       "      <td>ATL</td>\n",
       "      <td>Atlanta, GA</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>145.0</td>\n",
       "      <td>...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 1.0, 0.0)</td>\n",
       "      <td>(0.0, 1.0, 0.0, 0.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ABY</td>\n",
       "      <td>Albany, GA</td>\n",
       "      <td>ATL</td>\n",
       "      <td>Atlanta, GA</td>\n",
       "      <td>-9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>-9.000000</td>\n",
       "      <td>145.0</td>\n",
       "      <td>...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 1.0, 0.0)</td>\n",
       "      <td>(0.0, 1.0, 0.0, 0.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ABY</td>\n",
       "      <td>Albany, GA</td>\n",
       "      <td>ATL</td>\n",
       "      <td>Atlanta, GA</td>\n",
       "      <td>-9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.131181</td>\n",
       "      <td>145.0</td>\n",
       "      <td>...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 1.0, 0.0)</td>\n",
       "      <td>(0.0, 1.0, 0.0, 0.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ATL</td>\n",
       "      <td>Atlanta, GA</td>\n",
       "      <td>ABY</td>\n",
       "      <td>Albany, GA</td>\n",
       "      <td>24.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>145.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, ...</td>\n",
       "      <td>(0.0, 1.0, 0.0, 0.0)</td>\n",
       "      <td>(1.0, 0.0, 0.0, 0.0)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Origin OriginCityName Dest DestCityName  DepDelay  DepDelayMinutes  TaxiOut  \\\n",
       "0    ABY     Albany, GA  ATL  Atlanta, GA      -5.0              0.0     14.0   \n",
       "1    ABY     Albany, GA  ATL  Atlanta, GA      -5.0              0.0     13.0   \n",
       "2    ABY     Albany, GA  ATL  Atlanta, GA      -9.0              0.0     18.0   \n",
       "3    ABY     Albany, GA  ATL  Atlanta, GA     -12.0              0.0     17.0   \n",
       "4    ABY     Albany, GA  ATL  Atlanta, GA      -5.0              0.0     17.0   \n",
       "5    ABY     Albany, GA  ATL  Atlanta, GA       NaN              NaN     34.0   \n",
       "6    ABY     Albany, GA  ATL  Atlanta, GA       2.0              2.0     15.0   \n",
       "7    ABY     Albany, GA  ATL  Atlanta, GA      -9.0              0.0      7.0   \n",
       "8    ABY     Albany, GA  ATL  Atlanta, GA      -9.0              0.0     26.0   \n",
       "9    ATL    Atlanta, GA  ABY   Albany, GA      24.0             24.0     23.0   \n",
       "\n",
       "   TaxiIn   ArrDelay  Distance  ...  FlightWeek_index  arrDaypart_index  \\\n",
       "0     7.0  -8.000000     145.0  ...               8.0               2.0   \n",
       "1    12.0  -6.000000     145.0  ...               8.0               2.0   \n",
       "2    11.0  -2.000000     145.0  ...               8.0               2.0   \n",
       "3    11.0 -11.000000     145.0  ...               8.0               2.0   \n",
       "4    11.0  -1.000000     145.0  ...               8.0               2.0   \n",
       "5    13.0  22.000000     145.0  ...               8.0               2.0   \n",
       "6    10.0  -1.000000     145.0  ...              20.0               2.0   \n",
       "7    11.0  -9.000000     145.0  ...              20.0               2.0   \n",
       "8     8.0   4.131181     145.0  ...              20.0               2.0   \n",
       "9     3.0  22.000000     145.0  ...               7.0               1.0   \n",
       "\n",
       "   depDaypart_index                                      Origin_onehot  \\\n",
       "0               1.0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1               1.0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2               1.0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3               1.0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4               1.0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "5               1.0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "6               1.0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "7               1.0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "8               1.0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "9               0.0  (1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                               OriginCityName_onehot  \\\n",
       "0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "5  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "6  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "7  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "8  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "9  (0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                         Dest_onehot  \\\n",
       "0  (1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1  (1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2  (1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3  (1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4  (1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "5  (1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "6  (1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "7  (1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "8  (1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "9  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                 DestCityName_onehot  \\\n",
       "0  (0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1  (0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2  (0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3  (0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4  (0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "5  (0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "6  (0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "7  (0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "8  (0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "9  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                   FlightWeek_onehot     arrDaypart_onehot  \\\n",
       "0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...  (0.0, 0.0, 1.0, 0.0)   \n",
       "1  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...  (0.0, 0.0, 1.0, 0.0)   \n",
       "2  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...  (0.0, 0.0, 1.0, 0.0)   \n",
       "3  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...  (0.0, 0.0, 1.0, 0.0)   \n",
       "4  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...  (0.0, 0.0, 1.0, 0.0)   \n",
       "5  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...  (0.0, 0.0, 1.0, 0.0)   \n",
       "6  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  (0.0, 0.0, 1.0, 0.0)   \n",
       "7  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  (0.0, 0.0, 1.0, 0.0)   \n",
       "8  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  (0.0, 0.0, 1.0, 0.0)   \n",
       "9  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, ...  (0.0, 1.0, 0.0, 0.0)   \n",
       "\n",
       "      depDaypart_onehot  \n",
       "0  (0.0, 1.0, 0.0, 0.0)  \n",
       "1  (0.0, 1.0, 0.0, 0.0)  \n",
       "2  (0.0, 1.0, 0.0, 0.0)  \n",
       "3  (0.0, 1.0, 0.0, 0.0)  \n",
       "4  (0.0, 1.0, 0.0, 0.0)  \n",
       "5  (0.0, 1.0, 0.0, 0.0)  \n",
       "6  (0.0, 1.0, 0.0, 0.0)  \n",
       "7  (0.0, 1.0, 0.0, 0.0)  \n",
       "8  (0.0, 1.0, 0.0, 0.0)  \n",
       "9  (1.0, 0.0, 0.0, 0.0)  \n",
       "\n",
       "[10 rows x 33 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_df.limit(10).toPandas().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "33c5470d-8d00-4e07-9bb6-82c7ce999af4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Origin',\n",
       " 'OriginCityName',\n",
       " 'Dest',\n",
       " 'DestCityName',\n",
       " 'DepDelay',\n",
       " 'DepDelayMinutes',\n",
       " 'TaxiOut',\n",
       " 'TaxiIn',\n",
       " 'ArrDelay',\n",
       " 'Distance',\n",
       " 'CarrierDelay',\n",
       " 'WeatherDelay',\n",
       " 'NASDelay',\n",
       " 'SecurityDelay',\n",
       " 'LateAircraftDelay',\n",
       " 'FlightWeek',\n",
       " 'Year',\n",
       " 'depDaypart',\n",
       " 'arrDaypart',\n",
       " 'Origin_index',\n",
       " 'OriginCityName_index',\n",
       " 'Dest_index',\n",
       " 'DestCityName_index',\n",
       " 'FlightWeek_index',\n",
       " 'arrDaypart_index',\n",
       " 'depDaypart_index',\n",
       " 'Origin_onehot',\n",
       " 'OriginCityName_onehot',\n",
       " 'Dest_onehot',\n",
       " 'DestCityName_onehot',\n",
       " 'FlightWeek_onehot',\n",
       " 'arrDaypart_onehot',\n",
       " 'depDaypart_onehot']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "29dce33d-6157-4eb3-9ea1-2662df7357f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "req_cols = [\n",
    "     'Year',\n",
    "     'Origin_onehot',\n",
    "     'OriginCityName_onehot',\n",
    "     'Dest_onehot',\n",
    "     'DestCityName_onehot',\n",
    "     'FlightWeek_onehot',\n",
    "     'arrDaypart_onehot',\n",
    "     'depDaypart_onehot',\n",
    "     'DepDelay',\n",
    "     'DepDelayMinutes',\n",
    "     'TaxiOut',\n",
    "     'TaxiIn',\n",
    "     'Distance',\n",
    "     'CarrierDelay',\n",
    "     'WeatherDelay',\n",
    "     'NASDelay',\n",
    "     'SecurityDelay',\n",
    "     'LateAircraftDelay',\n",
    "     'ArrDelay',\n",
    "]\n",
    "\n",
    "useable_df = encoded_df[req_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "19846e26-4275-4b9d-b60e-4815098c286f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Year',\n",
       " 'Origin_onehot',\n",
       " 'OriginCityName_onehot',\n",
       " 'Dest_onehot',\n",
       " 'DestCityName_onehot',\n",
       " 'FlightWeek_onehot',\n",
       " 'arrDaypart_onehot',\n",
       " 'depDaypart_onehot',\n",
       " 'DepDelay',\n",
       " 'DepDelayMinutes',\n",
       " 'TaxiOut',\n",
       " 'TaxiIn',\n",
       " 'Distance',\n",
       " 'CarrierDelay',\n",
       " 'WeatherDelay',\n",
       " 'NASDelay',\n",
       " 'SecurityDelay',\n",
       " 'LateAircraftDelay']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "useable_df.columns[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cb5b8bb6-5bad-4428-ba4b-d833e48bb958",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "output_path = \"/Users/shreyasmac/Documents/Notes/Big Data/Final Project/delays_preprocessed_updated\"\n",
    "# Save the DataFrame to a CSV file\n",
    "delays_preprocessed.write.csv(output_path, header=True, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfa8869-d27b-47a4-ad4f-7648867f5f50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e647fc9-6c7f-411f-9a82-ea570b141b27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc85eeab-f4be-4b49-9cd7-a2cac9624b4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "60ecd260-3c8b-4202-b7a1-33854011bdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVE AFTER THIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3015d2-a61e-4c3a-9071-d005f4947eb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3671a6b-f87e-4eb3-9497-91829caf2d5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3670f3df-0492-4d8a-b826-cffa7b5b08f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c76f9b-f8da-4224-a598-c3d361054e71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "4d9e11b4-ce82-44ea-bfaa-94c310a890fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df = useable_df.filter(col(\"Year\") != 2022)\n",
    "test_data_df = useable_df.filter(col(\"Year\") == 2022)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "3bda8ae8-d7a8-49b5-bcbd-701c4e96ae31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "469a4fbc-d7d3-4280-a894-5449bebf5f8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/shreyasmac/Documents/Notes/Big Data/Final Project'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "59b7bec4-4b35-42f3-9748-d703e29b27ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a1b612-7eab-492a-8d0a-2b37c80d8e70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "333a5314-a3e7-4a49-a972-c89b45165485",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "\n",
    "# Combine features into a single vector column\n",
    "trainAssembler = VectorAssembler(inputCols=train_data_df.columns[:-1],\n",
    "                            outputCol=\"features\")\n",
    "trainOutput = trainAssembler.transform(train_data_df)\n",
    "\n",
    "\n",
    "testAssembler = VectorAssembler(inputCols=test_data_df.columns[:-1],\n",
    "                            outputCol=\"features\")\n",
    "testOutput = testAssembler.transform(test_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "0f751de9-af6e-4553-b3f9-39c6a845ec73",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_abcd = trainOutput[[\"features\"]]\n",
    "test_abcd = testOutput[[\"features\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "709074c2-367a-42dc-ab81-bd638365ed9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|(1610,[0,273,655,...|\n",
      "|(1610,[0,273,655,...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_abcd.limit(2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "cdeb90ca-e59c-4031-9d0e-b300e62a1f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data:  24461389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 145:==================================================>  (111 + 5) / 116]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data:  3955126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_data = trainOutput\n",
    "test_data = testOutput\n",
    "\n",
    "print(\"Train data: \", train_data.count())\n",
    "print(\"Test data: \", test_data.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "2e4573e0-072e-4448-8ae6-d21a3963ed5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 148:==================================================>  (111 + 5) / 116]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|Year|\n",
      "+----+\n",
      "|2018|\n",
      "|2019|\n",
      "|2020|\n",
      "|2021|\n",
      "+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_data.select(col(\"Year\")).distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "58e72b33-5d98-458e-ad11-af09fc6ad1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 151:==================================================>  (111 + 5) / 116]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|Year|\n",
      "+----+\n",
      "|2022|\n",
      "+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "test_data.select(col(\"Year\")).distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "f87487cb-852b-4fe0-ae04-dd9a3be77c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/27 19:27:48 ERROR Executor: Exception in task 0.0 in stage 159.0 (TID 4549)\n",
      "org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (`VectorAssembler$$Lambda/0x000000980226e288`: (struct<Year_double_VectorAssembler_d8f1f30441b5:double,Origin_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,OriginCityName_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,Dest_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,DestCityName_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,FlightWeek_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,arrDaypart_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,depDaypart_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,DepDelay:double,DepDelayMinutes:double,TaxiOut:double,TaxiIn:double,Distance:double,CarrierDelay:double,WeatherDelay:double,NASDelay:double,SecurityDelay:double,LateAircraftDelay:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:198)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\n",
      "removing nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n",
      "\t... 20 more\n",
      "24/04/27 19:27:48 WARN TaskSetManager: Lost task 0.0 in stage 159.0 (TID 4549) (10-18-165-231.dynapool.wireless.nyu.edu executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (`VectorAssembler$$Lambda/0x000000980226e288`: (struct<Year_double_VectorAssembler_d8f1f30441b5:double,Origin_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,OriginCityName_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,Dest_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,DestCityName_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,FlightWeek_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,arrDaypart_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,depDaypart_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,DepDelay:double,DepDelayMinutes:double,TaxiOut:double,TaxiIn:double,Distance:double,CarrierDelay:double,WeatherDelay:double,NASDelay:double,SecurityDelay:double,LateAircraftDelay:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:198)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\n",
      "removing nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n",
      "\t... 20 more\n",
      "\n",
      "24/04/27 19:27:48 ERROR TaskSetManager: Task 0 in stage 159.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o2621.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 159.0 failed 1 times, most recent failure: Lost task 0.0 in stage 159.0 (TID 4549) (10-18-165-231.dynapool.wireless.nyu.edu executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (`VectorAssembler$$Lambda/0x000000980226e288`: (struct<Year_double_VectorAssembler_d8f1f30441b5:double,Origin_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,OriginCityName_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,Dest_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,DestCityName_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,FlightWeek_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,arrDaypart_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,depDaypart_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,DepDelay:double,DepDelayMinutes:double,TaxiOut:double,TaxiIn:double,Distance:double,CarrierDelay:double,WeatherDelay:double,NASDelay:double,SecurityDelay:double,LateAircraftDelay:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:198)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n\t... 20 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4332)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3537)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n\tat jdk.internal.reflect.GeneratedMethodAccessor138.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (`VectorAssembler$$Lambda/0x000000980226e288`: (struct<Year_double_VectorAssembler_d8f1f30441b5:double,Origin_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,OriginCityName_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,Dest_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,DestCityName_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,FlightWeek_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,arrDaypart_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,depDaypart_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,DepDelay:double,DepDelayMinutes:double,TaxiOut:double,TaxiIn:double,Distance:double,CarrierDelay:double,WeatherDelay:double,NASDelay:double,SecurityDelay:double,LateAircraftDelay:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:198)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\t... 1 more\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n\t... 20 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[112], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainOutput\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/sql/dataframe.py:945\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    886\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[1;32m    887\u001b[0m \n\u001b[1;32m    888\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    943\u001b[0m \u001b[38;5;124;03m    name | Bob\u001b[39;00m\n\u001b[1;32m    944\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 945\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_show_string(n, truncate, vertical))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/sql/dataframe.py:963\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    957\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    958\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    959\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    960\u001b[0m     )\n\u001b[1;32m    962\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 963\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mshowString(n, \u001b[38;5;241m20\u001b[39m, vertical)\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    965\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o2621.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 159.0 failed 1 times, most recent failure: Lost task 0.0 in stage 159.0 (TID 4549) (10-18-165-231.dynapool.wireless.nyu.edu executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (`VectorAssembler$$Lambda/0x000000980226e288`: (struct<Year_double_VectorAssembler_d8f1f30441b5:double,Origin_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,OriginCityName_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,Dest_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,DestCityName_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,FlightWeek_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,arrDaypart_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,depDaypart_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,DepDelay:double,DepDelayMinutes:double,TaxiOut:double,TaxiIn:double,Distance:double,CarrierDelay:double,WeatherDelay:double,NASDelay:double,SecurityDelay:double,LateAircraftDelay:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:198)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n\t... 20 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4332)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3537)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n\tat jdk.internal.reflect.GeneratedMethodAccessor138.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (`VectorAssembler$$Lambda/0x000000980226e288`: (struct<Year_double_VectorAssembler_d8f1f30441b5:double,Origin_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,OriginCityName_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,Dest_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,DestCityName_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,FlightWeek_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,arrDaypart_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,depDaypart_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,DepDelay:double,DepDelayMinutes:double,TaxiOut:double,TaxiIn:double,Distance:double,CarrierDelay:double,WeatherDelay:double,NASDelay:double,SecurityDelay:double,LateAircraftDelay:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:198)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\t... 1 more\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n\t... 20 more\n"
     ]
    }
   ],
   "source": [
    "trainOutput.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "3468640e-2731-4d00-8e92-8d97e8a413c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/27 19:26:11 ERROR Executor: Exception in task 0.0 in stage 155.0 (TID 4537)\n",
      "org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (`VectorAssembler$$Lambda/0x000000980226e288`: (struct<Year_double_VectorAssembler_9660347001c1:double,Origin_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,OriginCityName_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,Dest_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,DestCityName_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,FlightWeek_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,arrDaypart_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,depDaypart_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,DepDelay:double,DepDelayMinutes:double,TaxiOut:double,TaxiIn:double,Distance:double,CarrierDelay:double,WeatherDelay:double,NASDelay:double,SecurityDelay:double,LateAircraftDelay:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:198)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n",
      "\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n",
      "\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1226)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2492)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\n",
      "removing nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n",
      "\t... 28 more\n",
      "24/04/27 19:26:11 WARN TaskSetManager: Lost task 0.0 in stage 155.0 (TID 4537) (10-18-165-231.dynapool.wireless.nyu.edu executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (`VectorAssembler$$Lambda/0x000000980226e288`: (struct<Year_double_VectorAssembler_9660347001c1:double,Origin_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,OriginCityName_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,Dest_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,DestCityName_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,FlightWeek_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,arrDaypart_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,depDaypart_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,DepDelay:double,DepDelayMinutes:double,TaxiOut:double,TaxiIn:double,Distance:double,CarrierDelay:double,WeatherDelay:double,NASDelay:double,SecurityDelay:double,LateAircraftDelay:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:198)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n",
      "\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n",
      "\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1226)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2492)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\n",
      "removing nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n",
      "\t... 28 more\n",
      "\n",
      "24/04/27 19:26:11 ERROR TaskSetManager: Task 0 in stage 155.0 failed 1 times; aborting job\n",
      "24/04/27 19:26:11 WARN TaskSetManager: Lost task 2.0 in stage 155.0 (TID 4539) (10-18-165-231.dynapool.wireless.nyu.edu executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 0 in stage 155.0 failed 1 times, most recent failure: Lost task 0.0 in stage 155.0 (TID 4537) (10-18-165-231.dynapool.wireless.nyu.edu executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (`VectorAssembler$$Lambda/0x000000980226e288`: (struct<Year_double_VectorAssembler_9660347001c1:double,Origin_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,OriginCityName_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,Dest_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,DestCityName_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,FlightWeek_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,arrDaypart_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,depDaypart_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,DepDelay:double,DepDelayMinutes:double,TaxiOut:double,TaxiIn:double,Distance:double,CarrierDelay:double,WeatherDelay:double,NASDelay:double,SecurityDelay:double,LateAircraftDelay:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:198)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n",
      "\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n",
      "\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1226)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2492)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\n",
      "removing nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n",
      "\t... 28 more\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/04/27 19:26:11 ERROR Instrumentation: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 155.0 failed 1 times, most recent failure: Lost task 0.0 in stage 155.0 (TID 4537) (10-18-165-231.dynapool.wireless.nyu.edu executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (`VectorAssembler$$Lambda/0x000000980226e288`: (struct<Year_double_VectorAssembler_9660347001c1:double,Origin_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,OriginCityName_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,Dest_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,DestCityName_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,FlightWeek_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,arrDaypart_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,depDaypart_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,DepDelay:double,DepDelayMinutes:double,TaxiOut:double,TaxiIn:double,Distance:double,CarrierDelay:double,WeatherDelay:double,NASDelay:double,SecurityDelay:double,LateAircraftDelay:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:198)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n",
      "\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n",
      "\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1226)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2492)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\n",
      "removing nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n",
      "\t... 28 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2493)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$1(RDD.scala:1228)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.RDD.aggregate(RDD.scala:1221)\n",
      "\tat org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:125)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:274)\n",
      "\tat org.apache.spark.ml.regression.RandomForestRegressor.$anonfun$train$1(RandomForestRegressor.scala:158)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:136)\n",
      "\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:45)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:78)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (`VectorAssembler$$Lambda/0x000000980226e288`: (struct<Year_double_VectorAssembler_9660347001c1:double,Origin_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,OriginCityName_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,Dest_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,DestCityName_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,FlightWeek_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,arrDaypart_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,depDaypart_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,DepDelay:double,DepDelayMinutes:double,TaxiOut:double,TaxiIn:double,Distance:double,CarrierDelay:double,WeatherDelay:double,NASDelay:double,SecurityDelay:double,LateAircraftDelay:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:198)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n",
      "\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n",
      "\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1226)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2492)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\t... 1 more\n",
      "Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\n",
      "removing nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n",
      "\t... 28 more\n",
      "\n",
      "24/04/27 19:26:11 WARN TaskSetManager: Lost task 7.0 in stage 155.0 (TID 4544) (10-18-165-231.dynapool.wireless.nyu.edu executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 0 in stage 155.0 failed 1 times, most recent failure: Lost task 0.0 in stage 155.0 (TID 4537) (10-18-165-231.dynapool.wireless.nyu.edu executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (`VectorAssembler$$Lambda/0x000000980226e288`: (struct<Year_double_VectorAssembler_9660347001c1:double,Origin_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,OriginCityName_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,Dest_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,DestCityName_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,FlightWeek_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,arrDaypart_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,depDaypart_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,DepDelay:double,DepDelayMinutes:double,TaxiOut:double,TaxiIn:double,Distance:double,CarrierDelay:double,WeatherDelay:double,NASDelay:double,SecurityDelay:double,LateAircraftDelay:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:198)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n",
      "\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n",
      "\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1226)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2492)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\n",
      "removing nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n",
      "\t... 28 more\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/04/27 19:26:11 WARN TaskSetManager: Lost task 6.0 in stage 155.0 (TID 4543) (10-18-165-231.dynapool.wireless.nyu.edu executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 0 in stage 155.0 failed 1 times, most recent failure: Lost task 0.0 in stage 155.0 (TID 4537) (10-18-165-231.dynapool.wireless.nyu.edu executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (`VectorAssembler$$Lambda/0x000000980226e288`: (struct<Year_double_VectorAssembler_9660347001c1:double,Origin_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,OriginCityName_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,Dest_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,DestCityName_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,FlightWeek_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,arrDaypart_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,depDaypart_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,DepDelay:double,DepDelayMinutes:double,TaxiOut:double,TaxiIn:double,Distance:double,CarrierDelay:double,WeatherDelay:double,NASDelay:double,SecurityDelay:double,LateAircraftDelay:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:198)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n",
      "\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n",
      "\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1226)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2492)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\n",
      "removing nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n",
      "\t... 28 more\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/04/27 19:26:11 WARN TaskSetManager: Lost task 3.0 in stage 155.0 (TID 4540) (10-18-165-231.dynapool.wireless.nyu.edu executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 0 in stage 155.0 failed 1 times, most recent failure: Lost task 0.0 in stage 155.0 (TID 4537) (10-18-165-231.dynapool.wireless.nyu.edu executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (`VectorAssembler$$Lambda/0x000000980226e288`: (struct<Year_double_VectorAssembler_9660347001c1:double,Origin_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,OriginCityName_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,Dest_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,DestCityName_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,FlightWeek_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,arrDaypart_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,depDaypart_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,DepDelay:double,DepDelayMinutes:double,TaxiOut:double,TaxiIn:double,Distance:double,CarrierDelay:double,WeatherDelay:double,NASDelay:double,SecurityDelay:double,LateAircraftDelay:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:198)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n",
      "\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n",
      "\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1226)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2492)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\n",
      "removing nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n",
      "\t... 28 more\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/04/27 19:26:11 WARN TaskSetManager: Lost task 5.0 in stage 155.0 (TID 4542) (10-18-165-231.dynapool.wireless.nyu.edu executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 0 in stage 155.0 failed 1 times, most recent failure: Lost task 0.0 in stage 155.0 (TID 4537) (10-18-165-231.dynapool.wireless.nyu.edu executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (`VectorAssembler$$Lambda/0x000000980226e288`: (struct<Year_double_VectorAssembler_9660347001c1:double,Origin_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,OriginCityName_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,Dest_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,DestCityName_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,FlightWeek_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,arrDaypart_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,depDaypart_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,DepDelay:double,DepDelayMinutes:double,TaxiOut:double,TaxiIn:double,Distance:double,CarrierDelay:double,WeatherDelay:double,NASDelay:double,SecurityDelay:double,LateAircraftDelay:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:198)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n",
      "\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n",
      "\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1226)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2492)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\n",
      "removing nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n",
      "\t... 28 more\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/04/27 19:26:11 WARN TaskSetManager: Lost task 4.0 in stage 155.0 (TID 4541) (10-18-165-231.dynapool.wireless.nyu.edu executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 0 in stage 155.0 failed 1 times, most recent failure: Lost task 0.0 in stage 155.0 (TID 4537) (10-18-165-231.dynapool.wireless.nyu.edu executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (`VectorAssembler$$Lambda/0x000000980226e288`: (struct<Year_double_VectorAssembler_9660347001c1:double,Origin_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,OriginCityName_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,Dest_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,DestCityName_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,FlightWeek_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,arrDaypart_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,depDaypart_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,DepDelay:double,DepDelayMinutes:double,TaxiOut:double,TaxiIn:double,Distance:double,CarrierDelay:double,WeatherDelay:double,NASDelay:double,SecurityDelay:double,LateAircraftDelay:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:198)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n",
      "\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n",
      "\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1226)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2492)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\n",
      "removing nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n",
      "\t... 28 more\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/04/27 19:26:11 WARN TaskSetManager: Lost task 1.0 in stage 155.0 (TID 4538) (10-18-165-231.dynapool.wireless.nyu.edu executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 0 in stage 155.0 failed 1 times, most recent failure: Lost task 0.0 in stage 155.0 (TID 4537) (10-18-165-231.dynapool.wireless.nyu.edu executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (`VectorAssembler$$Lambda/0x000000980226e288`: (struct<Year_double_VectorAssembler_9660347001c1:double,Origin_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,OriginCityName_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,Dest_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,DestCityName_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,FlightWeek_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,arrDaypart_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,depDaypart_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,DepDelay:double,DepDelayMinutes:double,TaxiOut:double,TaxiIn:double,Distance:double,CarrierDelay:double,WeatherDelay:double,NASDelay:double,SecurityDelay:double,LateAircraftDelay:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:198)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n",
      "\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n",
      "\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1226)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2492)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\n",
      "removing nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n",
      "\t... 28 more\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/04/27 19:26:11 WARN TaskSetManager: Lost task 8.0 in stage 155.0 (TID 4545) (10-18-165-231.dynapool.wireless.nyu.edu executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 0 in stage 155.0 failed 1 times, most recent failure: Lost task 0.0 in stage 155.0 (TID 4537) (10-18-165-231.dynapool.wireless.nyu.edu executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (`VectorAssembler$$Lambda/0x000000980226e288`: (struct<Year_double_VectorAssembler_9660347001c1:double,Origin_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,OriginCityName_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,Dest_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,DestCityName_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,FlightWeek_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,arrDaypart_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,depDaypart_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,DepDelay:double,DepDelayMinutes:double,TaxiOut:double,TaxiIn:double,Distance:double,CarrierDelay:double,WeatherDelay:double,NASDelay:double,SecurityDelay:double,LateAircraftDelay:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:198)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n",
      "\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n",
      "\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1226)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2492)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\n",
      "removing nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n",
      "\t... 28 more\n",
      "\n",
      "Driver stacktrace:)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o2506.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 155.0 failed 1 times, most recent failure: Lost task 0.0 in stage 155.0 (TID 4537) (10-18-165-231.dynapool.wireless.nyu.edu executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (`VectorAssembler$$Lambda/0x000000980226e288`: (struct<Year_double_VectorAssembler_9660347001c1:double,Origin_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,OriginCityName_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,Dest_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,DestCityName_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,FlightWeek_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,arrDaypart_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,depDaypart_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,DepDelay:double,DepDelayMinutes:double,TaxiOut:double,TaxiIn:double,Distance:double,CarrierDelay:double,WeatherDelay:double,NASDelay:double,SecurityDelay:double,LateAircraftDelay:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:198)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1226)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2492)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n\t... 28 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2493)\n\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$1(RDD.scala:1228)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.aggregate(RDD.scala:1221)\n\tat org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:125)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:274)\n\tat org.apache.spark.ml.regression.RandomForestRegressor.$anonfun$train$1(RandomForestRegressor.scala:158)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:136)\n\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:45)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:78)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (`VectorAssembler$$Lambda/0x000000980226e288`: (struct<Year_double_VectorAssembler_9660347001c1:double,Origin_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,OriginCityName_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,Dest_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,DestCityName_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,FlightWeek_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,arrDaypart_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,depDaypart_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,DepDelay:double,DepDelayMinutes:double,TaxiOut:double,TaxiIn:double,Distance:double,CarrierDelay:double,WeatherDelay:double,NASDelay:double,SecurityDelay:double,LateAircraftDelay:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:198)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1226)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2492)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\t... 1 more\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n\t... 28 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[105], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m rf \u001b[38;5;241m=\u001b[39m RandomForestRegressor(featuresCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m, labelCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArrDelay\u001b[39m\u001b[38;5;124m\"\u001b[39m, numTrees\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m rf\u001b[38;5;241m.\u001b[39mfit(train_data)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[1;32m      8\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtransform(test_data)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_java(dataset)\n\u001b[1;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj\u001b[38;5;241m.\u001b[39mfit(dataset\u001b[38;5;241m.\u001b[39m_jdf)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o2506.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 155.0 failed 1 times, most recent failure: Lost task 0.0 in stage 155.0 (TID 4537) (10-18-165-231.dynapool.wireless.nyu.edu executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (`VectorAssembler$$Lambda/0x000000980226e288`: (struct<Year_double_VectorAssembler_9660347001c1:double,Origin_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,OriginCityName_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,Dest_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,DestCityName_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,FlightWeek_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,arrDaypart_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,depDaypart_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,DepDelay:double,DepDelayMinutes:double,TaxiOut:double,TaxiIn:double,Distance:double,CarrierDelay:double,WeatherDelay:double,NASDelay:double,SecurityDelay:double,LateAircraftDelay:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:198)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1226)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2492)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n\t... 28 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2493)\n\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$1(RDD.scala:1228)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.aggregate(RDD.scala:1221)\n\tat org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:125)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:274)\n\tat org.apache.spark.ml.regression.RandomForestRegressor.$anonfun$train$1(RandomForestRegressor.scala:158)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:136)\n\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:45)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:78)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (`VectorAssembler$$Lambda/0x000000980226e288`: (struct<Year_double_VectorAssembler_9660347001c1:double,Origin_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,OriginCityName_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,Dest_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,DestCityName_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,FlightWeek_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,arrDaypart_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,depDaypart_onehot:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,DepDelay:double,DepDelayMinutes:double,TaxiOut:double,TaxiIn:double,Distance:double,CarrierDelay:double,WeatherDelay:double,NASDelay:double,SecurityDelay:double,LateAircraftDelay:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:198)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1226)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2492)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\t... 1 more\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n\t... 28 more\n"
     ]
    }
   ],
   "source": [
    "# Create a RandomForestRegressor\n",
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"ArrDelay\", numTrees=10)\n",
    "\n",
    "# Train the model\n",
    "model = rf.fit(train_data)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = RegressionEvaluator(labelCol=\"ArrDelay\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7942f6-12f4-4ebf-8744-d8d2b4d082d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ec50f3-0037-4eb5-94a1-4aefb74c5976",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5114a245-39a5-4138-adc0-7939cff9cf76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
